---
# YAML template -- requires manual editing, particularly with regard to out_offset and processors
# Generated for MAX78000 with input format HWC

arch: ai85netextrasmall
dataset: IMU_AI

layers:
  # Layer 0
  - name: trans_conv1_ConvTranspose_8
    # input shape: (30, 6, 6)
    data_format: HWC
    processors: 0x000000003fffffff
    out_offset: 0x4000
    op: ConvTranspose2d
    kernel_size: 3x3
    pad: 1
    activate: None
    # output shape: (16, 12, 12)

  # Layer 1
  - name: trans_conv2_ConvTranspose_6
    # input shape: (16, 12, 12)
    processors: 0x0000ffff00000000
    out_offset: 0x0000
    op: ConvTranspose2d
    kernel_size: 3x3
    pad: 1
    activate: None
    # output shape: (8, 24, 24)

  # Layer 2
  - name: conv1_Conv_6  # conv1_Conv_6 fused with conv1.activate
    # input shape: (8, 24, 24)
    processors: 0xff00000000000000
    out_offset: 0x4000
    op: Conv2d
    kernel_size: 3x3
    pad: 1
    activate: Relu
    # output shape: (8, 24, 24)

  # Layer 3
  - name: conv2_Conv_6  # conv2.pool and conv2_Conv_6 fused with conv2.activate
    # input shape: (8, 24, 24)
    processors: 0x00ff000000000000
    out_offset: 0x0000
    op: Conv2d
    kernel_size: 3x3
    pad: 1
    activate: Relu
    max_pool: 2
    pool_stride: 2
    pool_dilation: [1, 1]
    # output shape: (8, 12, 12)

  # Layer 4
  - name: conv3_Conv_6  # conv3.pool and conv3_Conv_6 fused with conv3.activate
    # input shape: (8, 12, 12)
    processors: 0x00000000000000ff
    out_offset: 0x4000
    op: Conv2d
    kernel_size: 3x3
    pad: 1
    activate: Relu
    max_pool: 4
    pool_stride: 4
    pool_dilation: [1, 1]
    # output shape: (5, 3, 3)

  # Layer 5
  - name: fc_Gemm_4
    # input shape: (5, 3, 3)
    processors: 0x0000000000001f00
    out_offset: 0x0000
    op: Linear
    flatten: true
    activate: None
    # output shape: (5,)
